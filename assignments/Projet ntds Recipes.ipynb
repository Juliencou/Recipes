{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NTDS'19 Project: A graph approach of cooking\n",
    "Couyoup√©trou Julien, Renaud David and Mueller Gauthier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives and description of the project\n",
    "\n",
    "\n",
    "The project is made of x parts:\n",
    "    blabla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sgt'></a>\n",
    "## Part I: Loading and cleaning the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from scipy.spatial.distance import pdist, squareform\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import json\n",
    "#from pygsp.graphs import TwoMoons\n",
    "#%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'ingredients': [{'text': '6 ounces penne'}, {'text': '2 cups Beechers Flagship Cheese Sauce (recipe follows)'}, {'text': '1 ounce Cheddar, grated (1/4 cup)'}, {'text': '1 ounce Gruyere cheese, grated (1/4 cup)'}, {'text': '1/4 to 1/2 teaspoon chipotle chili powder (see Note)'}, {'text': '1/4 cup (1/2 stick) unsalted butter'}, {'text': '1/3 cup all-purpose flour'}, {'text': '3 cups milk'}, {'text': '14 ounces semihard cheese (page 23), grated (about 3 1/2 cups)'}, {'text': '2 ounces semisoft cheese (page 23), grated (1/2 cup)'}, {'text': '1/2 teaspoon kosher salt'}, {'text': '1/4 to 1/2 teaspoon chipotle chili powder'}, {'text': '1/8 teaspoon garlic powder'}, {'text': '(makes about 4 cups)'}], 'url': 'http://www.epicurious.com/recipes/food/views/-world-s-best-mac-and-cheese-387747', 'partition': 'train', 'title': 'Worlds Best Mac and Cheese', 'id': '000018c8a5', 'instructions': [{'text': 'Preheat the oven to 350 F. Butter or oil an 8-inch baking dish.'}, {'text': 'Cook the penne 2 minutes less than package directions.'}, {'text': '(It will finish cooking in the oven.)'}, {'text': 'Rinse the pasta in cold water and set aside.'}, {'text': 'Combine the cooked pasta and the sauce in a medium bowl and mix carefully but thoroughly.'}, {'text': 'Scrape the pasta into the prepared baking dish.'}, {'text': 'Sprinkle the top with the cheeses and then the chili powder.'}, {'text': 'Bake, uncovered, for 20 minutes.'}, {'text': 'Let the mac and cheese sit for 5 minutes before serving.'}, {'text': 'Melt the butter in a heavy-bottomed saucepan over medium heat and whisk in the flour.'}, {'text': 'Continue whisking and cooking for 2 minutes.'}, {'text': 'Slowly add the milk, whisking constantly.'}, {'text': 'Cook until the sauce thickens, about 10 minutes, stirring frequently.'}, {'text': 'Remove from the heat.'}, {'text': 'Add the cheeses, salt, chili powder, and garlic powder.'}, {'text': 'Stir until the cheese is melted and all ingredients are incorporated, about 3 minutes.'}, {'text': 'Use immediately, or refrigerate for up to 3 days.'}, {'text': 'This sauce reheats nicely on the stove in a saucepan over low heat.'}, {'text': 'Stir frequently so the sauce doesnt scorch.'}, {'text': 'This recipe can be assembled before baking and frozen for up to 3 monthsjust be sure to use a freezer-to-oven pan and increase the baking time to 50 minutes.'}, {'text': 'One-half teaspoon of chipotle chili powder makes a spicy mac, so make sure your family and friends can handle it!'}, {'text': 'The proportion of pasta to cheese sauce is crucial to the success of the dish.'}, {'text': 'It will look like a lot of sauce for the pasta, but some of the liquid will be absorbed.'}]}, {'ingredients': [{'text': '1 c. elbow macaroni'}, {'text': '1 c. cubed American cheese (4 ounce.)'}, {'text': '1/2 c. sliced celery'}, {'text': '1/2 c. minced green pepper'}, {'text': '3 tbsp. minced pimento'}, {'text': '1/2 c. mayonnaise or possibly salad dressing'}, {'text': '1 tbsp. vinegar'}, {'text': '3/4 teaspoon salt'}, {'text': '1/2 teaspoon dry dill weed'}], 'url': 'http://cookeatshare.com/recipes/dilly-macaroni-salad-49166', 'partition': 'train', 'title': 'Dilly Macaroni Salad Recipe', 'id': '000033e39b', 'instructions': [{'text': 'Cook macaroni according to package directions; drain well.'}, {'text': 'Cold.'}, {'text': 'Combine macaroni, cheese cubes, celery, green pepper and pimento.'}, {'text': 'Blend together mayonnaise or possibly salad dressing, vinegar, salt and dill weed; add in to macaroni mix.'}, {'text': 'Toss lightly.'}, {'text': 'Cover and refrigeratewell.'}, {'text': 'Serve salad in lettuce lined bowl if you like.'}, {'text': 'Makes 6 servings.'}]}, {'ingredients': [{'text': '8 tomatoes, quartered'}, {'text': 'Kosher salt'}, {'text': '1 red onion, cut into small dice'}, {'text': '1 green bell pepper, cut into small dice'}, {'text': '1 red bell pepper, cut into small dice'}, {'text': '1 yellow bell pepper, cut into small dice'}, {'text': '1/2 cucumber, cut into small dice'}, {'text': 'Extra-virgin olive oil, for drizzling'}, {'text': '3 leaves fresh basil, finely chopped'}], 'url': 'http://www.foodnetwork.com/recipes/gazpacho1.html', 'partition': 'train', 'title': 'Gazpacho', 'id': '000035f7ed', 'instructions': [{'text': 'Add the tomatoes to a food processor with a pinch of salt and puree until smooth.'}, {'text': 'Combine the onions, bell peppers and cucumbers with the tomato puree in a large bowl.'}, {'text': 'Chill at least 1 hour.'}, {'text': 'Drizzle with olive oil, garnish with chopped basil and serve.'}]}, {'ingredients': [{'text': '2 12 cups milk'}, {'text': '1 12 cups water'}, {'text': '14 cup butter'}, {'text': 'mashed potatoes, 1 box, homestyle'}, {'text': '1 (8 ounce) can whole kernel corn (drained)'}, {'text': '1 cup cheddar cheese'}, {'text': '1 cup French-fried onions'}], 'url': 'http://www.food.com/recipe/crunchy-onion-potato-bake-479149', 'partition': 'test', 'title': 'Crunchy Onion Potato Bake', 'id': '00003a70b1', 'instructions': [{'text': 'Preheat oven to 350 degrees Fahrenheit.'}, {'text': 'Spray pan with non stick cooking spray.'}, {'text': 'Heat milk, water and butter to boiling; stir in contents of both pouches of potatoes; let stand one minute.'}, {'text': 'Stir in corn.'}, {'text': 'Spoon half the potato mixture in pan.'}, {'text': 'Sprinkle half each of cheese and onions; top with remaining potatoes.'}, {'text': 'Sprinkle with remaining cheese and onions.'}, {'text': 'Bake 10 to 15 minutes until cheese is melted.'}, {'text': 'Enjoy !'}]}, {'ingredients': [{'text': '1 (3 ounce) package watermelon gelatin'}, {'text': '14 cup boiling water'}, {'text': '1 (12 ounce) package Cool Whip, thawed'}, {'text': '2 cups cubed seedless watermelon'}, {'text': '1 graham cracker crust'}], 'url': 'http://www.food.com/recipe/cool-n-easy-creamy-watermelon-pie-66340', 'partition': 'train', 'title': \"Cool 'n Easy Creamy Watermelon Pie\", 'id': '00004320bb', 'instructions': [{'text': 'Dissolve Jello in boiling water.'}, {'text': 'Allow to cool to room temp.'}, {'text': 'Whisk in Cool Whip.'}, {'text': 'Fold in watermelon.'}, {'text': 'Spoon into crust.'}, {'text': 'Chill for 2-3 hours or overnight.'}, {'text': 'Yum!'}]}, {'ingredients': [{'text': '12 cup shredded coconut'}, {'text': '1 lb lean ground beef'}, {'text': '1 -2 tablespoon minced fresh garlic (or to taste)'}, {'text': 'salt and black pepper'}, {'text': '1 tablespoon lemon juice'}, {'text': '1 tablespoon soy sauce'}, {'text': '2 tablespoons cornstarch'}, {'text': '1 (8 ounce) can pineapple chunks, drained, reserving the liquid'}, {'text': '1 (16 ounce) can mandarin oranges, drained, reserving the liquid'}, {'text': '12 cup cashew nuts'}], 'url': 'http://www.food.com/recipe/easy-tropical-beef-skillet-75863', 'partition': 'train', 'title': 'Easy Tropical Beef Skillet', 'id': '0000631d90', 'instructions': [{'text': 'In a large skillet, toast the coconut over medium heat, until golden and crisp; set aside.'}, {'text': 'Brown ground beef and garlic in the same skillet; drain well.'}, {'text': 'Add salt, pepper lemon juice and soy sauce.'}, {'text': 'In a small bowl combine the cornstarch with reserved pineapple and mandarin orange liquids; stir well until smooth then add to ground beef and cook over medium heat for 5 mins, stirring constantly, until mixture is thickened.'}, {'text': 'Stir in the pineapple and mandarin oranges; cook 2-3 mins, or until thoroughly heated.'}, {'text': 'Serve over noodles or rice, and sprinkle with more toasted coconut and cashew nuts.'}]}, {'ingredients': [{'text': '2 Chicken thighs'}, {'text': '2 tsp Kombu tea'}, {'text': '1 White pepper'}], 'url': 'https://cookpad.com/us/recipes/150100-kombu-tea-grilled-chicken-thigh', 'partition': 'train', 'title': 'Kombu Tea Grilled Chicken Thigh', 'id': '000075604a', 'instructions': [{'text': 'Pierce the skin of the chicken with a fork or knife.'}, {'text': 'Sprinkle with kombu tea evenly on both sides of the chicken, about 1 teaspoon per chicken thigh.'}, {'text': 'Brown the skin side of the chicken first over high heat until golden brown.'}, {'text': 'Sprinkle some pepper on the meat just before flipping over.'}, {'text': 'Then brown the other side until golden brown.'}]}, {'ingredients': [{'text': '6 -8 cups fresh rhubarb, or'}, {'text': '6 -8 cups frozen rhubarb, thawed'}, {'text': '1 12 cups granulated sugar'}, {'text': '6 ounces strawberry Jell-O gelatin dessert'}, {'text': '1 white cake mix (2 layer size)'}, {'text': '1 12 cups water'}, {'text': '12 cup butter or 12 cup margarine, melted'}], 'url': 'http://www.food.com/recipe/strawberry-rhubarb-dump-cake-408694', 'partition': 'train', 'title': 'Strawberry Rhubarb Dump Cake', 'id': '00007bfd16', 'instructions': [{'text': 'Put ingredients in a buttered 9 x 12 x 2-inch pan in even layers in the order that they are given - DO NOT MIX.'}, {'text': 'Bake in a 350 oven for 1 hour.'}]}, {'ingredients': [{'text': '8 ounces, weight Light Fat Free Vanilla Yogurt (I Used Activia)'}, {'text': '1 cup Fresh Sliced Strawberries'}, {'text': '1/4 cups Low-fat Granola'}], 'url': 'http://tastykitchen.com/recipes/breakfastbrunch/yogurt-parfaits/', 'partition': 'train', 'title': 'Yogurt Parfaits', 'id': '000095fc1d', 'instructions': [{'text': 'Layer all ingredients in a serving dish.'}]}, {'ingredients': [{'text': '2 cups flour'}, {'text': '1 tablespoon cinnamon'}, {'text': '2 teaspoons baking soda'}, {'text': '1 teaspoon salt'}, {'text': '14 teaspoon baking powder'}, {'text': '3 eggs'}, {'text': '2 cups sugar'}, {'text': '1 cup vegetable oil'}, {'text': '1 tablespoon vanilla'}, {'text': '2 cups grated raw zucchini'}, {'text': '1 cup walnuts'}], 'url': 'http://www.food.com/recipe/zucchini-nut-bread-329682', 'partition': 'train', 'title': 'Zucchini Nut Bread', 'id': '0000973574', 'instructions': [{'text': 'Sift dry ingredients.'}, {'text': 'beat eggs untill frothy, add sugar, oil and vanilla.'}, {'text': 'Beat untillthick.'}, {'text': 'Stir in zucchini.'}, {'text': 'blend in sifted dry ingredients.'}, {'text': 'Fold in nuts.'}, {'text': 'pour in greased loaf pan and bake at 350 degrees for 1 1/2 hours.'}]}]\n"
     ]
    }
   ],
   "source": [
    "with open('RecipeDataset/layer1.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "print(data[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1: Graph construction\n",
    "Build a similarity graph using the euclidean distance between data points.   \n",
    "**Note:** Use an RBF kernel to set the edge weights $w_{ij}=\\exp(-||x_i- x_j||_2^2 / ~ 2 \\sigma^2)$ of your adjacency and threshold the ones with the smallest magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def scale_nutr(ingr, factor):\n",
    "    for k, v in ingr.items():\n",
    "        ingr[k] = v * factor\n",
    "\n",
    "\n",
    "def update_energy(recipe):\n",
    "    nutr = {'nrg': 'energy', 'fat': 'fat', 'pro': 'protein', 'sat': 'saturates', 'sug': 'sugars'}\n",
    "    tot_weight = sum(recipe['weight_per_ingr'])\n",
    "    for k, v in nutr.items():\n",
    "        tot = sum(map(lambda x: x[k], recipe['nutr_per_ingredient']))\n",
    "        recipe['nutr_values_per100g'][v] = tot * 100 / tot_weight\n",
    "    # salt needs to be handled separately: values in nutr_per_ingredients are in mg of sodium\n",
    "    # and they are converted into salt quantities for the nutr_values_per_100g using the rule that\n",
    "    # 100 g of salt has 40 g of sodium\n",
    "    sod = sum(map(lambda x: x['sod'], recipe['nutr_per_ingredient'])) / 1000  # convert to g\n",
    "    recipe['nutr_values_per100g']['salt'] = (sod * 100 / tot_weight) / .4\n",
    "\n",
    "\n",
    "# cf. https://www.nutrition.org.uk/healthyliving/helpingyoueatwell/324-labels.html\n",
    "def update_nutr_score(recipe):\n",
    "    if recipe['nutr_values_per100g']['salt'] <= 0.3:\n",
    "        recipe['fsa_lights_per100g']['salt'] = 'green'\n",
    "    elif recipe['nutr_values_per100g']['salt'] <= 1.5:\n",
    "        recipe['fsa_lights_per100g']['salt'] = 'orange'\n",
    "    else:\n",
    "        recipe['fsa_lights_per100g']['salt'] = 'red'\n",
    "\n",
    "    if recipe['nutr_values_per100g']['fat'] <= 3:\n",
    "        recipe['fsa_lights_per100g']['fat'] = 'green'\n",
    "    elif recipe['nutr_values_per100g']['fat'] <= 17.5:\n",
    "        recipe['fsa_lights_per100g']['fat'] = 'orange'\n",
    "    else:\n",
    "        recipe['fsa_lights_per100g']['fat'] = 'red'\n",
    "\n",
    "    if recipe['nutr_values_per100g']['saturates'] <= 1.5:\n",
    "        recipe['fsa_lights_per100g']['saturates'] = 'green'\n",
    "    elif recipe['nutr_values_per100g']['saturates'] <= 5:\n",
    "        recipe['fsa_lights_per100g']['saturates'] = 'orange'\n",
    "    else:\n",
    "        recipe['fsa_lights_per100g']['saturates'] = 'red'\n",
    "\n",
    "    if recipe['nutr_values_per100g']['sugars'] <= 5:\n",
    "        recipe['fsa_lights_per100g']['sugars'] = 'green'\n",
    "    elif recipe['nutr_values_per100g']['sugars'] <= 22.5:\n",
    "        recipe['fsa_lights_per100g']['sugars'] = 'orange'\n",
    "    else:\n",
    "        recipe['fsa_lights_per100g']['sugars'] = 'red'\n",
    "\n",
    "\n",
    "def restore_fractions(recipe):\n",
    "    out = deepcopy(recipe)\n",
    "    units = map(lambda x: x['text'], recipe['unit'])\n",
    "    qty = map(lambda x: x['text'], recipe['quantity'])\n",
    "    weights = recipe['weight_per_ingr']\n",
    "    qty_units = list(zip(qty, units, weights))\n",
    "\n",
    "    out['check'] = False # flag for further checking\n",
    "    # print(qty_units)\n",
    "    for k in range(len(qty_units)):\n",
    "        p = qty_units[k]\n",
    "        if p[1] != 'cup' and p[1] != 'teaspoon' and p[1] != 'tablespoon':\n",
    "            continue\n",
    "        factor = 1\n",
    "        if p[0] == '12':  # 12 -> 1/2, divide by 24 nutritional stuff\n",
    "            q = '1/2'\n",
    "            factor = 1 / 24\n",
    "        elif p[0] == '13':  # 13 -> 1/3, divide by 39\n",
    "            q = '1/3'\n",
    "            factor = 1 / 39\n",
    "        elif p[0] == '14':  # 14 -> 1/4, divide by 56 nutritional stuff\n",
    "            q = '1/4'\n",
    "            factor = 1 / 56\n",
    "        elif p[0] == '18':  # 18 -> 1/8 divide by 144\n",
    "            q = '1/8'\n",
    "            factor = 1 / 144\n",
    "        elif p[0] == '23':  # 23 -> 2/3 divide by 34.5\n",
    "            q = '2/3'\n",
    "            factor = 1 / 34.5\n",
    "        elif p[0] == '34':  # 34 -> 3/4, divide by 136/3\n",
    "            q = '3/4'\n",
    "            factor = 3 / 136\n",
    "        elif p[0] == '38':  # 38 -> 3/8 divide by 304/3\n",
    "            q = '3/8'\n",
    "            factor = 3 / 304\n",
    "        elif p[0] == '58':  # 58 -> 5/8 divide 464/5\n",
    "            q = '5/8'\n",
    "            factor = 5 / 464\n",
    "        elif p[0] == '78':  # 78 -> 7/8 divide by 624/7\n",
    "            q = '7/8'\n",
    "            factor = 7 / 624\n",
    "        elif len(p[0]) == 2 and p[0] != '10' and p[0] != 20:\n",
    "            q = p[0]\n",
    "            # print('Needs checking: {} {}'.format(p[0], p[1]))\n",
    "            out['check'] = True\n",
    "        else:\n",
    "            q = p[0]\n",
    "        out['quantity'][k]['text'] = q\n",
    "        w = p[2] * factor\n",
    "        out['weight_per_ingr'][k] = w\n",
    "        scale_nutr(out['nutr_per_ingredient'][k], factor)\n",
    "\n",
    "    update_energy(out)\n",
    "    update_nutr_score(out)\n",
    "    return out\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with open('recipes_with_nutritional_info.json', 'r') as f:\n",
    "        recipes = json.load(f)\n",
    "    # build list of indices to check (do they contain weights bigger than 2kg)\n",
    "    check = []\n",
    "    for idx in tqdm(range(len(recipes))):\n",
    "        if any(t > 2000 for t in recipes[idx]['weight_per_ingr']):\n",
    "            check.append(idx)\n",
    "\n",
    "    for k in tqdm(check):\n",
    "        recipes[k] = restore_fractions(recipes[k])\n",
    "    # check which are flagged for manual review\n",
    "    flagged = [r for r in range(len(recipes)) if recipes[r].get('check', False)]\n",
    "\n",
    "    # write output\n",
    "    with open('recipes_with_nutritional_info_fixed_qty.json', 'w') as f:\n",
    "        json.dump(recipes, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': '6 ounces penne'}, {'text': '2 cups Beechers Flagship Cheese Sauce (recipe follows)'}, {'text': '1 ounce Cheddar, grated (1/4 cup)'}, {'text': '1 ounce Gruyere cheese, grated (1/4 cup)'}, {'text': '1/4 to 1/2 teaspoon chipotle chili powder (see Note)'}, {'text': '1/4 cup (1/2 stick) unsalted butter'}, {'text': '1/3 cup all-purpose flour'}, {'text': '3 cups milk'}, {'text': '14 ounces semihard cheese (page 23), grated (about 3 1/2 cups)'}, {'text': '2 ounces semisoft cheese (page 23), grated (1/2 cup)'}, {'text': '1/2 teaspoon kosher salt'}, {'text': '1/4 to 1/2 teaspoon chipotle chili powder'}, {'text': '1/8 teaspoon garlic powder'}, {'text': '(makes about 4 cups)'}]\n",
      "1029720\n"
     ]
    }
   ],
   "source": [
    "print(data[0][\"ingredients\"])\n",
    "print(len(data))\n",
    "\n",
    "\"\"\"\n",
    "To do:\n",
    "    Remove parenthesis\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency = epsilon_similarity_graph(X, sigma=None, epsilon=None)\n",
    "plt.spy(adjacency)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you choose `sigma`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you choose the threshold `epsilon`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2: Laplacian\n",
    "Build the combinatorial and normalized graph laplacians for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_laplacian(adjacency: np.ndarray, normalize: bool):\n",
    "    \"\"\" Return:\n",
    "        L (n x n ndarray): combinatorial or symmetric normalized Laplacian.\n",
    "    \"\"\"\n",
    "    # Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laplacian_comb = compute_laplacian(adjacency, normalize=False)\n",
    "laplacian_norm = compute_laplacian(adjacency, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3: Eigendecomposition\n",
    "\n",
    "For both Laplacian matrices, compute the eigendecomposition $L = U^\\top \\Lambda U$, where the columns $u_k \\in \\mathbb{R}^N$ of $U = [u_1, \\dots, u_N] \\in \\mathbb{R}^{N \\times N}$ are the eigenvectors and the diagonal elements $\\lambda_k = \\Lambda_{kk}$ are the corresponding eigenvalues. Make sure that the eigenvalues are ordered, i.e., $\\lambda_1 \\leq \\lambda_2 \\leq \\dots \\leq \\lambda_N$. \n",
    "\n",
    "Justify your choice of a solver for the eigendecomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_decomposition(laplacian: np.ndarray):\n",
    "    \"\"\" Return:\n",
    "        lamb (np.array): eigenvalues of the Laplacian\n",
    "        U (np.ndarray): corresponding eigenvectors.\n",
    "    \"\"\"\n",
    "    # Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamb_comb, U_comb = spectral_decomposition(laplacian_comb)\n",
    "lamb_norm, U_norm = spectral_decomposition(laplacian_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4: Interpretation\n",
    "We plot the sorted eigenvalues as a function of their index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(121)\n",
    "plt.plot(lamb_comb)\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Eigenvalue')\n",
    "plt.title('Eigenvalues $L_{comb}$')\n",
    "plt.subplot(122)\n",
    "plt.plot(lamb_norm)\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Eigenvalue')\n",
    "plt.title('Eigenvalues $L_{norm}$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the lowest eigenvalue $\\lambda_0$ and the corresponding eigenvector $u_0$? Answer for both Laplacian matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When filtering a signal or computing polynomials, which Laplacian provides the best numerical stability? Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5: Connected components\n",
    "The eigendecomposition provides an easy way to compute the number of connected components in the graph. Fill the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_number_connected_components(lamb: np.array, threshold: float):\n",
    "    \"\"\" lamb: array of eigenvalues of a Laplacian\n",
    "        Return:\n",
    "        n_components (int): number of connected components.\n",
    "    \"\"\"\n",
    "    # Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune the parameters $\\epsilon$ and $\\sigma$ of the similarity graph so that the graph is connected. Otherwise, clustering would be too simple!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compute_number_connected_components(lamb_norm, threshold=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral clustering\n",
    "\n",
    "Let us now see one application of spectral graph theory to clustering the two moon dataset.\n",
    "\n",
    "#### Question 6: Baseline\n",
    "\n",
    "As a baseline, let us first see how the simplest clustering algorithm, K-means, performs on this dataset. Use K-means to assign a cluster to each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Your code here\n",
    "y_pred =  # Vector with cluster assignments\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means cannot find a good solution to this problem. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 7: Spectral clustering\n",
    "\n",
    "As opposed to naive K-means, spectral clustering doesn't operate on the input space but on the eigenspace of the graph that represents the data. Implement spectral clustering. You can use \n",
    "[this tutorial](http://lasa.epfl.ch/teaching/lectures/ML_Phd/Notes/tutoSC.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectralClustering():\n",
    "    def __init__(self, n_classes: int, normalize: bool):\n",
    "        self.n_classes = n_classes\n",
    "        self.normalize = normalize\n",
    "        self.laplacian = None\n",
    "        self.e = None\n",
    "        self.U = None\n",
    "        self.clustering_method =  # Your code here\n",
    "        \n",
    "    def fit_predict(self, adjacency):\n",
    "        \"\"\" Your code should be correct both for the combinatorial\n",
    "            and the symmetric normalized spectral clustering.\n",
    "            Return:\n",
    "            y_pred (np.ndarray): cluster assignments.\n",
    "        \"\"\"\n",
    "        # Your code here\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Connected components:\", compute_number_connected_components(lamb_norm, threshold=1e-12))\n",
    "spectral_clustering = SpectralClustering(n_classes=2, normalize=True)\n",
    "y_pred = spectral_clustering.fit_predict(adjacency)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 8: On your dataset\n",
    "\n",
    "Can you think of another 2D dataset in which k-means would badly perform, but spectral clustering would not?\n",
    "Construct it!\n",
    "For this question you can import any dataset of your choice, for example from `sklearn.datasets` or `pygsp.graphs`, but you can also get creative and define something of your own. First, create and plot the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run K-means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the similarity graph, and run spectral clustering with both the combinatorial and normalized Laplacian matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Comment your results here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction with Laplacian Eigenmaps\n",
    "\n",
    "Most datasets are very high-dimensional, which means it can be very hard to understand their geometry. Fortunately, there exists multiple  techniques that can help us to reduce the dimensionality of the data, and allow us to visualize it. \n",
    "\n",
    "In this part of the assignment we will use MNIST to compare these techniques. Indeed, without dimensionality reduction it would be very difficult to answer questions like: are the different digits clustered together in different areas of space? \n",
    "\n",
    "But first, let's load our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_mnist\n",
    "\n",
    "X_mnist, y_mnist = load_mnist()\n",
    "classes = np.unique(y_mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 9: Laplacian eigenmaps\n",
    "\n",
    "Most dimensionality reduction algorithms are constructed such that some property of the dataset remains invariant in the lower dimensional representation. Before implementing laplacian eigenmaps, can you say what property of the data does this algorithm preserve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function that uses Laplacian eigenmaps to do dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplacian_eigenmaps(X:np.ndarray, dim: int, sigma: float, epsilon: float, normalize: bool):\n",
    "    \"\"\" Return:\n",
    "        coords (n x dim array): new coordinates for the data points.\"\"\"\n",
    "    # Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this function to visualize MNIST in 2D. Feel free to play with the different parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 2\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize MNIST in 3D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 3\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 10: Comparison with other methods  \n",
    "We provide the visualization of MNIST with other methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE, Isomap\n",
    "\n",
    "# This cell can take a few minutes to run\n",
    "run_this_cell = False\n",
    "\n",
    "if run_this_cell:\n",
    "    # In 2d\n",
    "    embeddings = [PCA(n_components=2, copy=True, whiten=True, tol=1e-5),\n",
    "                  Isomap(n_components=2, n_neighbors=5),\n",
    "                  TSNE(n_components=2)]\n",
    "\n",
    "    for embedding in embeddings:\n",
    "        X_embedded = embedding.fit_transform(X_mnist)\n",
    "        fig = plt.figure()\n",
    "        for i in classes:\n",
    "            mask = y_mnist == i\n",
    "            plt.scatter(X_embedded[mask, 0], X_embedded[mask, 1], label=i)\n",
    "        plt.legend()\n",
    "        plt.title('Embedding method: '+ type(embedding).__name__)\n",
    "        plt.show()\n",
    "\n",
    "    # In 3d\n",
    "    embeddings = [PCA(n_components=3, copy=True, whiten=True, tol=1e-5),\n",
    "                  Isomap(n_components=3, n_neighbors=5),\n",
    "                  TSNE(n_components=3)]\n",
    "\n",
    "    for embedding in embeddings:\n",
    "        X_embedded = embedding.fit_transform(X_mnist)\n",
    "        fig = plt.figure()\n",
    "        ax = Axes3D(fig)\n",
    "        for i in classes:\n",
    "            mask = y_mnist == i\n",
    "            ax.scatter(X_embedded[mask, 0], X_embedded[mask, 1], X_embedded[mask, 2], label=i)\n",
    "        ax.legend()\n",
    "        ax.title.set_text('Embedding method: '+ type(embedding).__name__)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a few words, what are the principles guiding the design of each method? Compare their results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gsp'></a>\n",
    "## Part II: Regularization on graphs with Graph Signal Processing\n",
    "\n",
    "In this part of the assignment we are going to familiarize ourselves with the main concepts in Graph Signal Processing and regularization on graphs in general. From now on, you can only use the following libraries as well as the functions that you implemented in the previous parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pygsp.graphs import Bunny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will use a nearest-neighbor graph constructed from the Stanford Bunny point cloud included in the PyGSP library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Bunny()\n",
    "adjacency = np.asarray(G.W.todense())\n",
    "n_nodes = adjacency.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the following function to plot our signals on this graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bunny(x=None, title='', vlim=[-0.03, 0.03]):\n",
    "    fig = plt.gcf()\n",
    "    ax = plt.gca()\n",
    "    if not isinstance(ax, Axes3D):\n",
    "        ax = plt.subplot(111, projection='3d')\n",
    "    if x is not None:\n",
    "        x = np.squeeze(x)\n",
    "\n",
    "    p = ax.scatter(G.coords[:,0], G.coords[:,1], G.coords[:,2], c=x, marker='o',\n",
    "                   s=5, cmap='RdBu_r', vmin=vlim[0], vmax=vlim[1])\n",
    "    ax.view_init(elev=-90, azim=90)\n",
    "    ax.dist = 7\n",
    "    ax.set_axis_off()\n",
    "    ax.set_title(title)\n",
    "    if x is not None:\n",
    "        fig.colorbar(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(111, projection='3d')\n",
    "plot_bunny()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 11: Graph frequencies\n",
    "\n",
    "Let us start by constructing the normalized graph laplacians from the adjacency matrix and find its spectral decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laplacian = compute_laplacian(adjacency, normalize=True)\n",
    "lam, U = spectral_decomposition(laplacian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(lam)\n",
    "plt.title('Eigenvalues $L_{norm}$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make things more clear we will plot some of its eigenvectors (0, 1, 3, 10, 100) as signals on the bunny graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 9))\n",
    "plt.subplot(231, projection='3d')\n",
    "plot_bunny(x=U[:,0], title='Eigenvector #0')\n",
    "plt.subplot(232, projection='3d')\n",
    "plot_bunny(x=U[:,1], title='Eigenvector #1')\n",
    "plt.subplot(233, projection='3d')\n",
    "plot_bunny(x=U[:,2], title='Eigenvector #2')\n",
    "\n",
    "plt.subplot(234, projection='3d')\n",
    "plot_bunny(x=U[:,3], title='Eigenvector #3')\n",
    "plt.subplot(235, projection='3d')\n",
    "plot_bunny(x=U[:,10], title='Eigenvector #10')\n",
    "plt.subplot(236, projection='3d')\n",
    "plot_bunny(x=U[:,100], title='Eigenvector #100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can you say in terms of the variation (smoothness) of these signals? How can the smoothness of a signal be measured?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 12: Graph Fourier Transform\n",
    "\n",
    "Create a function to compute the Graph Fourier Transform (GFT) of a graph signal and its inverse.\n",
    "**Note**: You can assume that you have internal access to the eigendecomposition (`U` and `lam`) of the laplacian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GFT(signal: np.ndarray):\n",
    "    # Your code here\n",
    "\n",
    "def iGFT(fourier_coefficients: np.ndarray):\n",
    "    # Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create a graph signal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = G.coords[:, 0] + G.coords[:, 1] + 3 * G.coords[:, 2]\n",
    "x /= np.linalg.norm(x) \n",
    "\n",
    "noise = np.random.randn(n_nodes)\n",
    "noise /= np.linalg.norm(noise) \n",
    "\n",
    "x_noisy = x + 0.3*noise\n",
    "\n",
    "plot_bunny(x_noisy, vlim=[min(x_noisy), max(x_noisy)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and plot its graph spectrum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(lam, np.abs(GFT(x_noisy)), 'r.') \n",
    "plt.plot(lam, np.abs(GFT(x)), 'g-')\n",
    "plt.xlabel('$\\lambda$')\n",
    "plt.ylabel('GFT')\n",
    "plt.legend(['$x_{noisy}$', '$x$'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 13: Graph filters\n",
    "\n",
    "We will try to extract the signal from the noise using graph filters. Let us start by creating three ideal graph filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_lp = np.ones((n_nodes,))\n",
    "ideal_bp = np.ones((n_nodes,))\n",
    "ideal_hp = np.ones((n_nodes,))\n",
    "\n",
    "ideal_lp[lam >= 0.1] = 0  # Low-pass filter with cut-off at lambda=0.1\n",
    "ideal_bp[lam < 0.1] = 0  # Band-pass filter with cut-offs at lambda=0.1 and lambda=0.5\n",
    "ideal_bp[lam > 0.5] = 0\n",
    "ideal_hp[lam <= 1] = 0  # High-pass filter with cut-off at lambda=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, create the ideal graph filter that implements the solution of Tikhonov regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.99 / np.max(lam)\n",
    "\n",
    "ideal_tk =  # Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the spectral responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lam, ideal_lp, '-', label='LP')\n",
    "plt.plot(lam, ideal_bp, '-', label='BP')\n",
    "plt.plot(lam, ideal_hp, '-', label='HP')\n",
    "plt.plot(lam, ideal_tk, '-', label='Tikhonov')\n",
    "plt.xlabel('$\\lambda$')\n",
    "plt.ylabel('Spectral response')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to filter a signal given an ideal graph filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ideal_graph_filter(x: np.ndarray, spectral_response: np.ndarray):\n",
    "    \"\"\"Return a filtered signal.\"\"\"\n",
    "    # Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_lp = ideal_graph_filter(x_noisy,ideal_lp)\n",
    "x_bp = ideal_graph_filter(x_noisy,ideal_bp)\n",
    "x_hp = ideal_graph_filter(x_noisy,ideal_hp)\n",
    "x_tk = ideal_graph_filter(x_noisy,ideal_tk)\n",
    "\n",
    "plt.figure(figsize=(18, 9))\n",
    "plt.subplot(231, projection='3d')\n",
    "plot_bunny(x=x, title='signal (true)', vlim=[min(x), max(x)])\n",
    "plt.subplot(232, projection='3d')\n",
    "plot_bunny(x=x_noisy, title='signal (noisy)', vlim=[min(x), max(x)])\n",
    "plt.subplot(233, projection='3d')\n",
    "plot_bunny(x=x_lp, title='Low-pass', vlim=[min(x_lp), max(x_lp)])\n",
    "plt.subplot(234, projection='3d')\n",
    "plot_bunny(x=x_bp, title='Band-pass', vlim=[min(x_bp), max(x_bp)])\n",
    "plt.subplot(235, projection='3d')\n",
    "plot_bunny(x=x_hp, title='High-pass', vlim=[min(x_hp), max(x_hp)])\n",
    "plt.subplot(236, projection='3d')\n",
    "plot_bunny(x=x_tk, title='Tikhonov denoised signal', vlim=[min(x_tk), max(x_tk)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would you link to the observations you made before about the spectral decomposition of the laplacian?\n",
    "Also, judging from the results, what type of model prior do you think Tikhonov regularization enforces?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 14: Polynomial graph filters\n",
    "\n",
    "We have seen how we can use the GFT to define different filters that enhance or reduce certain frequency bands. However, to do so, we require an explicit eigendecomposition of the graph laplacian, which has a cost $O(n^3)$. For very large graphs this is very intense computationally. We will now see how we can obtain similar results by filtering the signals directly without resorting to an eigendecomposition.\n",
    "\n",
    "The key idea is to use a polynomial of the graph laplacian to define a graph filter, i.e., $g(L)x=\\sum_{k=1}^K \\alpha_k L^k x$, and use the fact that the powers of a diagonalizable matrix can be written in terms of powers of its eigenvalues. This is\n",
    "$$\n",
    "L^k=(U\\Lambda U^T)^k=U\\Lambda^k U^T = U\\begin{bmatrix}\n",
    "(\\lambda_0)^k &\\dots & 0\\\\\n",
    "\\vdots & \\ddots & \\vdots\\\\\n",
    "0 & \\dots & (\\lambda_N)^k\n",
    "\\end{bmatrix} U^T.\n",
    "$$\n",
    "\n",
    "This means that a polynomial of the graph laplacian acts independently on each eigenvalue of the graph, and has a frequency spectrum of\n",
    "$$g(\\lambda)=\\sum_{k=1}^K \\alpha_k \\lambda^k.$$\n",
    "Hence,\n",
    "$$g(L)x=\\sum_{k=1}^K \\alpha_k L^k x=\\sum_{k=1}^K \\alpha_k U\\Lambda^k U^T x=U \\left(\\sum_{k=1}^K \\alpha_k\\Lambda^k \\right)U^T x=\\operatorname{iGFT}\\left(g(\\Lambda)\\operatorname{GFT}(x)\\right).$$\n",
    "\n",
    "With these ingredients, we have reduced the design of graph filters in the vertex domain to a regression task that approximates a given spectral response by a polynomial. There are multiple ways to do this, but in this assignment we will implement a very simple strategy based on [least-squares regression](https://en.wikipedia.org/wiki/Polynomial_regression#Matrix_form_and_calculation_of_estimates)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function to find the coefficients of a polynomial that approximates a given ideal filter.\n",
    "**Hint:** `np.vander` and `np.linalg.lstsq`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_polynomial(lam: np.ndarray, order: int, spectral_response: np.ndarray):\n",
    "    \"\"\" Return an array of polynomial coefficients of length 'order'.\"\"\"\n",
    "    # Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function to compute the frequency response of that filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_graph_filter_response(coeff: np.array, lam: np.ndarray):\n",
    "    \"\"\" Return an array of the same shape as lam.\n",
    "        response[i] is the spectral response at frequency lam[i]. \"\"\"\n",
    "    # Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us fit the Tikhonov ideal filter with several polynomials of different order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lam, ideal_tk)\n",
    "orders = [1, 2, 3, 5, 10, 20]\n",
    "for order in orders:    \n",
    "    coeff_tk = fit_polynomial(lam, order, ideal_tk)\n",
    "    plt.plot(lam, polynomial_graph_filter_response(coeff_tk, lam))\n",
    "\n",
    "plt.xlabel('$\\lambda$')\n",
    "plt.ylabel('Spectral response')\n",
    "plt.legend(orders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have only defined a way to compute the coefficients of our laplacian polynomial. Let us now compute our graph filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_graph_filter(coeff: np.array, laplacian: np.ndarray):\n",
    "    \"\"\" Return the laplacian polynomial with coefficients 'coeff'. \"\"\"\n",
    "    # Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the previous plot, choose a filter order that achieves (in your opinion) a good tradeoff in terms of computational complexity and response accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order =  # Your code here\n",
    "coeff_tk = fit_polynomial(lam, order, ideal_tk)\n",
    "g_tk = polynomial_graph_filter(coeff_tk, laplacian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 15: ARMA filter\n",
    "\n",
    "As you have seen in class, polynomial graph filters are only one of the ways in which you can approximate ideal graph filters. \n",
    "In this sense, ARMA filters are a natural way to implement Tikhonov denoising on graphs.\n",
    "Let us recall the general solution of the Tikhonov regularized denoising problem \n",
    "\n",
    "$$y=(I+\\alpha L)^{-1}x. $$\n",
    "\n",
    "With a little bit of algebra manipulation we can rewrite this expression as\n",
    "$$\n",
    "    y = -\\alpha L y + x,\n",
    "$$\n",
    "from which we can derive the iterative algorithm\n",
    "$$\n",
    "    y_k = -\\alpha L y_{k-1} + x\\qquad k=1,2,\\dots\n",
    "$$\n",
    "which is guaranteed to converge as long as $\\alpha \\lambda_{max} < 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the ARMA version of Tikhonov regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arma_tikhonov(x: np.ndarray, laplacian: np.ndarray, alpha: float, max_iter=50):\n",
    "    \"\"\" Return an array of the same shape as x.\"\"\"\n",
    "    # Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the previous noisy graph signal with the polynomial and ARMA approximations of the ideal Tikhonov filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tk_polynomial =  # Your code here\n",
    "x_tk_arma = arma_tikhonov(x_noisy, laplacian, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compare with the previous version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 4))\n",
    "plt.subplot(131, projection='3d')\n",
    "plot_bunny(x_tk, title='Ideal filter', vlim=[min(x_tk), max(x_tk)])\n",
    "plt.subplot(132, projection='3d')\n",
    "plot_bunny(x_tk_polynomial, title='Polynomial filter', vlim=[min(x_tk), max(x_tk)])\n",
    "plt.subplot(133, projection='3d')\n",
    "plot_bunny(x_tk_arma, title='ARMA filter', vlim=[min(x_tk), max(x_tk)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ml'></a>\n",
    "## Part III: Machine Learning on Graphs\n",
    "\n",
    "So far, we have only played with toy examples. Let us see the use of these tools in practice! In particular, let us see how we can use some graph filters to construct features to feed a classifier. For this part of the assignment we will import some extra packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import networkx as nx\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import dgl.function as fn\n",
    "from dgl import DGLGraph\n",
    "from dgl.data.citation_graph import load_cora\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the CORA dataset and the citation graph that we created in Assignment 1. However, to simplify the next tasks we will directly use the preprocessed version of this dataset contained within the Deep Graph Library (DGL).\n",
    "\n",
    "In this assignment, we will interpret CORA's features as multidimensional graph signals living on the citation graph.\n",
    "Our task is to design a classifier that uses these features and the geometry of the graph can identify the type of paper each node represents.\n",
    "\n",
    "The goal of this exercise is to do semi-supervised learning on graphs.\n",
    "We assume that we know to which scientific field a small subset of the papers belongs (the ones contained in `train_mask`).\n",
    "The goal is to predict to which field the other papers belong, using both the citation graph and the bag-of-word representation of each paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cora = load_cora()\n",
    "\n",
    "features = torch.FloatTensor(cora.features)    # Feature vector for each paper\n",
    "labels = torch.LongTensor(cora.labels)         # The field to which each paper belongs\n",
    "\n",
    "train_mask = torch.BoolTensor(cora.train_mask) # Mask of nodes selected for training\n",
    "val_mask = torch.BoolTensor(cora.val_mask)     # Mask of nodes selected for validation\n",
    "test_mask = torch.BoolTensor(cora.test_mask)   # Mask of nodes selected for testing\n",
    "\n",
    "in_feats = features.shape[1]\n",
    "n_classes = cora.num_labels\n",
    "n_edges = cora.graph.number_of_edges()\n",
    "\n",
    "graph = cora.graph\n",
    "adjacency = np.asarray(nx.to_numpy_matrix(graph))\n",
    "n_nodes = adjacency.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise we will use the normalized laplacian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laplacian = compute_laplacian(adjacency, normalize=True)\n",
    "lam, U = spectral_decomposition(laplacian)\n",
    "lam_max = np.max(lam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 16: Logistic regression\n",
    "\n",
    "The simplest classification method consists in ignoring the citation graph and trying to classify the papers using only the features.\n",
    "In this case, the problem is viewed as a standard classification task.\n",
    "To train our classifier we will select a few nodes in our graph for training and fit a [logistic regression classifier](https://en.wikipedia.org/wiki/Logistic_regression) on them.\n",
    "To avoid overfitting to the test set when we do hyperparameter tuning, we will also select a validation set.\n",
    "And finally, we will test our classifier on the rest of the nodes.\n",
    "**Hint:** use `sklearn.linear_model.LogisticRegression`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = features[train_mask]\n",
    "train_labels = labels[train_mask]\n",
    "val_features = features[val_mask]\n",
    "val_labels = labels[val_mask]\n",
    "test_features = features[test_mask]\n",
    "test_labels = labels[test_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a logistic regression model\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc =  # Your code here\n",
    "val_acc =  # Your code here\n",
    "test_acc =  # Your code here\n",
    "\n",
    "print('Train accuracy {:.4f} | Validation accuracy {:.4f} | Test accuracy {:.4f}'.format(train_acc, val_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 17: Handcrafted graph filters\n",
    "\n",
    "That's not a bad start! Now, let's try to improve a bit the results by taking into account the graph structure using tools from GSP. For this purpose, we will design a handcrafted filter that will be used to denoise the signal, before feeding it to a logistic regression.\n",
    "\n",
    "However, before we start, what hypothesis can you make on the spectral properties of the denoised signal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this prior, design an ideal filter response that you believe could enhance important features of the graph. \n",
    "\n",
    "**Note:** you just need to design one graph filter that we will apply to all features. Don't design a different filter for each feature. \n",
    "\n",
    "**Note:** finding the right filter can be very challenging, don't worry if you can't find it. Just make sure you experiment with a few configurations and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_filter =  # Store your spectral response here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose a filter order to approximate your filter using laplacian polynomials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order =  # Your code here\n",
    "\n",
    "coeff = fit_polynomial(lam, order, ideal_filter)\n",
    "graph_filter = polynomial_graph_filter(coeff, laplacian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the frequency response of your spectral template and its polynomial approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lam, ideal_filter)\n",
    "plt.plot(lam, polynomial_graph_filter_response(coeff, lam))\n",
    "plt.legend(['Ideal', 'Polynomial'])\n",
    "plt.xlabel('$\\lambda$')\n",
    "plt.ylabel('Spectral response')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create the new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_features = graph_filter @ features.numpy()\n",
    "\n",
    "train_features = filtered_features[train_mask,:]\n",
    "train_labels = labels[train_mask]\n",
    "\n",
    "val_features = filtered_features[val_mask,:]\n",
    "val_labels = labels[val_mask]\n",
    "\n",
    "test_features = filtered_features[test_mask,:]\n",
    "test_labels = labels[test_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train another logistic regression classifier on the new features. Remember to play with the regularization parameters to achieve a well performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc =  # Your code here\n",
    "val_acc =  # Your code here\n",
    "test_acc =  # Your code here\n",
    "\n",
    "print('Train accuracy {:.4f} | Validation accuracy {:.4f} | Test accuracy {:.4f}'.format(train_acc, val_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 18: Graph convolutional networks\n",
    "\n",
    "By now, you will probably have seen that it is challenging to find the right combination of spectral response, filter parameters and regularization method. And in most cases, this is a painstaking job. Wouldn't it be great to automate these tasks?\n",
    "\n",
    "Fortunately, this is possible if we use the right tools! Specifically, we will see that Graph Convolutional Networks are a great framework to automatize the feature extraction method.\n",
    "\n",
    "In this exercise, we will follow the same classification pipeline as above, but instead of hand-crafting our filter we will let `PyTorch` find the coefficients for us using gradient descent.\n",
    "\n",
    "In this section, most of the code is already written. Try to understand it and to play with some parameters. It may be useful if you want to solve some learning task in your project.\n",
    "\n",
    "We start by constructing a `LaplacianPolynomial` model in `DGL`. It computes the function: $f(X) = \\sum_{i=1}^{k} \\alpha_i L^i X \\theta$ where the trainable parameters are the coefficients $\\alpha_i$ and the matrix $\\theta$. This function can be interpreted as a filtering of $X$ by $\\sum_{i=1}^{k} \\alpha_i L^i$ followed by a linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaplacianPolynomial(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_feats: int,\n",
    "                 out_feats: int,\n",
    "                 k: int,\n",
    "                 dropout_prob: float,\n",
    "                 norm=True):\n",
    "        super().__init__()\n",
    "        self._in_feats = in_feats\n",
    "        self._out_feats = out_feats\n",
    "        self._k = k\n",
    "        self._norm = norm\n",
    "        # Contains the weights learned by the Laplacian polynomial\n",
    "        self.pol_weights = nn.Parameter(torch.Tensor(self._k + 1))\n",
    "        # Contains the weights learned by the logistic regression (without bias)\n",
    "        self.logr_weights = nn.Parameter(torch.Tensor(in_feats, out_feats))\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Reinitialize learnable parameters.\"\"\"\n",
    "        torch.manual_seed(0)\n",
    "        torch.nn.init.xavier_uniform_(self.logr_weights, gain=0.01)\n",
    "        torch.nn.init.normal_(self.pol_weights, mean=0.0, std=1e-3)\n",
    "\n",
    "    def forward(self, graph, feat):\n",
    "        r\"\"\"Compute graph convolution.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        * Input shape: :math:`(N, *, \\text{in_feats})` where * means any number of additional\n",
    "          dimensions, :math:`N` is the number of nodes.\n",
    "        * Output shape: :math:`(N, *, \\text{out_feats})` where all but the last dimension are\n",
    "          the same shape as the input.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        graph (DGLGraph) : The graph.\n",
    "        feat (torch.Tensor): The input feature\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        (torch.Tensor) The output feature\n",
    "        \"\"\"\n",
    "        feat = self.dropout(feat)\n",
    "        graph = graph.local_var()\n",
    "        \n",
    "        # D^(-1/2)\n",
    "        norm = torch.pow(graph.in_degrees().float().clamp(min=1), -0.5)\n",
    "        shp = norm.shape + (1,) * (feat.dim() - 1)\n",
    "        norm = torch.reshape(norm, shp)\n",
    "\n",
    "        # mult W first to reduce the feature size for aggregation.\n",
    "        feat = torch.matmul(feat, self.logr_weights)\n",
    "\n",
    "        result = self.pol_weights[0] * feat.clone()\n",
    "\n",
    "        for i in range(1, self._k + 1):\n",
    "            old_feat = feat.clone()\n",
    "            if self._norm:\n",
    "                feat = feat * norm\n",
    "            graph.ndata['h'] = feat\n",
    "            # Feat is not modified in place\n",
    "            graph.update_all(fn.copy_src(src='h', out='m'),\n",
    "                             fn.sum(msg='m', out='h'))\n",
    "            if self._norm:\n",
    "                graph.ndata['h'] = graph.ndata['h'] * norm\n",
    "\n",
    "            feat = old_feat - graph.ndata['h']\n",
    "            result += self.pol_weights[i] * feat\n",
    "\n",
    "        return result\n",
    "\n",
    "    def extra_repr(self):\n",
    "        \"\"\"Set the extra representation of the module,\n",
    "        which will come into effect when printing the model.\n",
    "        \"\"\"\n",
    "        summary = 'in={_in_feats}, out={_out_feats}'\n",
    "        summary += ', normalization={_norm}'\n",
    "        return summary.format(**self.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have are model ready we just need to create a function that performs one step of our training loop, and another one that evaluates our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, g, features, labels, loss_fcn, train_mask, optimizer):\n",
    "    model.train()  # Activate dropout\n",
    "    \n",
    "    logits = model(g, features)\n",
    "    loss = loss_fcn(logits[train_mask], labels[train_mask])\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "def evaluate(model, g, features, labels, mask):\n",
    "    model.eval()  # Deactivate dropout\n",
    "    with torch.no_grad():\n",
    "        logits = model(g, features)[mask]  # only compute the evaluation set\n",
    "        labels = labels[mask]\n",
    "        _, indices = torch.max(logits, dim=1)\n",
    "        correct = torch.sum(indices == labels)\n",
    "        return correct.item() * 1.0 / len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol_order = 3\n",
    "lr = 0.2\n",
    "weight_decay = 5e-6\n",
    "n_epochs = 1000\n",
    "p_dropout = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And train the classifier end to end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = DGLGraph(cora.graph)\n",
    "\n",
    "model = LaplacianPolynomial(in_feats, n_classes, pol_order, p_dropout)\n",
    "\n",
    "loss_fcn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=lr,\n",
    "                             weight_decay=weight_decay)\n",
    "\n",
    "dur = []\n",
    "for epoch in range(n_epochs):\n",
    "    if epoch >= 3:\n",
    "        t0 = time.time()\n",
    "    loss = train(model, graph, features, labels, loss_fcn, train_mask, optimizer)\n",
    "\n",
    "    if epoch >= 3:\n",
    "        dur.append(time.time() - t0)\n",
    "\n",
    "    acc = evaluate(model, graph, features, labels, val_mask)\n",
    "    print(\"Epoch {:05d} | Time(s) {:.4f} | Train Loss {:.4f} | Val Accuracy {:.4f}\". format(\n",
    "            epoch, np.mean(dur), loss.item(), acc))\n",
    "\n",
    "print()\n",
    "acc = evaluate(model, graph, features, labels, test_mask)\n",
    "print(\"Test Accuracy {:.4f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trained this way our GCN based on polynomials of the laplacian is a black box. Fortunately, however, the only difference between this shallow model and our previous classifier is the way we chose the filter coefficients.\n",
    "\n",
    "Let's see what the network learned.\n",
    "Print the coefficients of the learned filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_gcn =  # Your code here\n",
    "print(coeff_gcn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To interpret the model we can plot the frequency response of the learned filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(lam, np.abs(polynomial_graph_filter_response(coeff_gcn, lam)))\n",
    "plt.xlabel('$\\lambda$')\n",
    "plt.ylabel('Spectral response (db)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 19\n",
    "\n",
    "As we said, the whole classification pipeline of the previous exercise is identical to the one we tried before: Graph filtering + Logistic regression. The only difference lies in the way we chose the filter coefficients. First we were choosing them manually, and now, we let `PyTorch` find them for us. However, if everything is correct we should be able to use this filter to construct new hand-crafted features and train a logistic regression model that achieves good accuracy on the training set. Let's do that!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the learned coefficients to train a new feature extractor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_gcn_filter =  # Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract the new features by filtering the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_gcn = graph_gcn_filter @ features.numpy()\n",
    "\n",
    "train_features_gcn = features_gcn[train_mask,:]\n",
    "train_labels = labels[train_mask]\n",
    "val_features_gcn = features_gcn[val_mask,:]\n",
    "val_labels = labels[val_mask]\n",
    "test_features_gcn = features_gcn[test_mask,:]\n",
    "test_labels = labels[test_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a logistic regression on these features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's evaluate this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc =  # Your code here\n",
    "val_acc =  # Your code here\n",
    "test_acc =  # Your code here\n",
    "\n",
    "print('Train accuracy {:.4f} | Validation accuracy {:.4f} | Test accuracy {:.4f}'.format(train_acc, val_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of this model may not be exactly the same as the one obtained with Pytorch. What are the differences in the training procedure that can explain this gap?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
